# LangGraph workflow configuration with LLM model settings
models:
  # Default model configuration used as fallback
  default:
    model_name: "gpt-4.1-mini"
    temperature: 0.1
    max_tokens: 4000
  
  # Node-specific model configurations
  nodes:
    input_processing:
      model_name: "gpt-4.1-mini"
      temperature: 0.0
      max_tokens: 1500
    
    generating_content:
      model_name: "gpt-4.1-mini"
      temperature: 0.2
      max_tokens: 8000
    
    recognition_handwritten:
      model_name: "gpt-4.1-mini"
      temperature: 0.1
      max_tokens: 6000
    
    synthesis_material:
      model_name: "gpt-4.1-mini"
      temperature: 0.1
      max_tokens: 8000
    
    generating_questions:
      model_name: "gpt-4.1-mini"
      temperature: 0.3
      max_tokens: 3000
    
    answer_question:
      model_name: "gpt-4.1-mini"
      temperature: 0.2
      max_tokens: 6000

output_config:
  save_format: "markdown"
  output_directory: "./data/outputs" 