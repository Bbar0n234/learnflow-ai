# Нефункциональные требования (NFR) LearnFlow AI
# TODO: актуализировать

## 1. Введение

LearnFlow AI - образовательная система генерации учебных материалов на базе LangGraph, развертываемая пользователями локально. Система ориентирована на обработку экзаменационных вопросов и рукописных конспектов для создания персонализированных учебных материалов с использованием больших языковых моделей.

### Ключевые принципы
- **Локальное развертывание** - система развертывается пользователем на собственной инфраструктуре
- **Качество превыше скорости** - приоритет на качество генерируемого контента
- **Техническая доступность** - баланс между функциональностью и простотой развертывания

## 2. Качественные атрибуты (ISO/IEC 25010)

### 2.1 Производительность (Performance)

#### Временные характеристики
- **Приоритет**: НИЗКИЙ
- **Целевые показатели**:
  - Обработка одного экзаменационного вопроса: < 60 секунд
  - Распознавание рукописной страницы: < 30 секунд
  - Генерация gap-вопросов: < 45 секунд
  - Полный цикл обработки материала: < 5 минут
- **Обоснование**: Система не критична по времени отклика, качество важнее скорости

#### Использование ресурсов
- **Приоритет**: СРЕДНИЙ
- **Требования**:
  - RAM: минимум 4 GB, рекомендовано 8 GB
  - CPU: 2+ ядра для параллельной обработки
  - Дисковое пространство: 5 GB для артефактов и логов
  - Поддержка Docker и Docker Compose
- **Лимиты**:
  - Максимальный размер изображения: 10 MB
  - Максимум изображений за запрос: 10
  - Размер файла артефакта: 10 MB

### 2.2 Надежность (Reliability)

#### Отказоустойчивость
- **Приоритет**: ВЫСОКИЙ
- **Стратегия**: Graceful Degradation
- **Требования**:
  - При сбое узла распознавания система продолжает работу без конспектов
  - При недоступности GitHub артефакты сохраняются локально
  - При сбое LangFuse продолжается работа без трассировки
  - HITL (Human-in-the-Loop) таймауты с автоматическим продолжением

#### Восстанавливаемость
- **Приоритет**: СРЕДНИЙ
- **Механизмы**:
  - Checkpointing через PostgreSQL для сохранения состояния workflow
  - Атомарная запись файлов артефактов
  - Автоматический restart контейнеров через Docker
  - Логирование всех критических операций для анализа

### 2.3 Удобство использования (Usability)

#### Простота установки
- **Приоритет**: ВЫСОКИЙ
- **Требования**:
  - One-command запуск через `./run.sh` или `docker-compose up`
  - Автоматическая проверка зависимостей
  - Ясные сообщения об ошибках конфигурации
  - Документированный процесс настройки

#### Обучаемость
- **Приоритет**: ВЫСОКИЙ
- **Обеспечение**:
  - README с quick start инструкциями
  - Примеры использования в `_experiments/`
  - Интерактивная документация API через Swagger
  - Telegram bot с понятным интерфейсом

### 2.4 Безопасность (Security)

#### Защита от инъекций
- **Приоритет**: КРИТИЧЕСКИЙ
- **Механизмы**:
  - SecurityGuard для валидации пользовательского ввода
  - Fuzzy matching для обнаружения prompt injections (порог 0.85)
  - Санитизация контента перед обработкой LLM
  - Валидация размера и типа загружаемых файлов

#### Конфиденциальность
- **Приоритет**: ВЫСОКИЙ
- **Требования**:
  - Локальное хранение всех данных
  - Безопасное управление API ключами через `.env`
  - Отсутствие логирования чувствительной информации
  - Изоляция данных между thread_id

### 2.5 Совместимость (Compatibility)

#### Интероперабельность
- **Приоритет**: СРЕДНИЙ
- **Интерфейсы**:
  - REST API для интеграции с внешними системами
  - Telegram Bot API для мобильного доступа
  - GitHub API для хранения артефактов
  - LangFuse API для мониторинга
  - Поддержка стандартных форматов изображений (JPG, PNG)

### 2.6 Сопровождаемость (Maintainability)

#### Модульность
- **Приоритет**: ВЫСОКИЙ
- **Архитектурные решения**:
  - Слабая связанность через интерфейсы узлов
  - Разделение на независимые сервисы (bot, learnflow, artifacts)
  - Конфигурация через YAML без изменения кода
  - Dependency Injection для моделей и сервисов

#### Тестируемость
- **Приоритет**: СРЕДНИЙ
- **Обеспечение**:
  - Изолированные узлы workflow для unit-тестирования
  - Моковые конфигурации для тестов
  - Примеры в `_experiments/` как интеграционные тесты
  - Логирование с trace_id для отладки

#### Анализируемость
- **Приоритет**: КРИТИЧЕСКИЙ
- **Инструментарий**:
  - Структурированное логирование с уровнями
  - LangFuse для трассировки LLM вызовов
  - Метрики использования токенов и времени выполнения
  - Health check endpoints для мониторинга
  - Визуализация workflow через LangGraph Studio

### 2.7 Переносимость (Portability)

#### Адаптируемость
- **Приоритет**: СРЕДНИЙ
- **Требования**:
  - Контейнеризация через Docker
  - Поддержка Linux, macOS, Windows (через Docker)
  - Конфигурируемые пути и порты
  - UV для управления зависимостями Python

## 3. Архитектурные ограничения

### Технологический стек
- **Обязательные**: Python 3.11+, Docker, Docker Compose
- **Фреймворки**: FastAPI, LangGraph, LangChain
- **LLM провайдер**: OpenAI API (возможность замены через конфигурацию)
- **Хранилища**: PostgreSQL (checkpointer), локальная файловая система

### Масштабируемость
- **Текущая модель**: Вертикальное масштабирование (single-node)
- **Будущее**: Подготовка к горизонтальному масштабированию через:
  - Stateless узлы workflow
  - Внешнее хранилище состояния
  - Message queue ready архитектура
  - Разделение на микросервисы

## 4. Операционные требования

### Мониторинг и наблюдаемость
- **Приоритет**: КРИТИЧЕСКИЙ
- **Компоненты**:
  - Централизованное логирование с ротацией
  - LangFuse dashboards для LLM метрик
  - Health checks для всех сервисов
  - Алерты при критических ошибках (в перспективе)

### Развертывание
- **Среды**:
  - Development: локальный запуск через UV
  - Production: Docker Compose orchestration
- **CI/CD готовность**: 
  - Dockerfile для каждого сервиса
  - Environment-based конфигурация
  - Версионирование через git tags

## 5. Метрики успеха

### Качественные метрики
- Точность распознавания рукописного текста > 85%
- Релевантность генерируемых вопросов > 90%
- Полнота покрытия темы в материалах > 80%

### Операционные метрики
- Успешное развертывание с первой попытки > 80%
- Время до первого результата < 10 минут после установки
- Доступность сервиса > 95% (для локального развертывания)

### Пользовательские метрики
- Время освоения системы < 30 минут
- Количество обращений к документации < 3 на установку
- Успешная обработка материалов без вмешательства > 70%

## 6. Эволюционные требования

### Краткосрочная перспектива (3-6 месяцев)
- Поддержка дополнительных LLM провайдеров (Anthropic, Google)
- Расширение форматов входных данных (PDF, DOCX)
- Web UI для альтернативы Telegram боту
- Интеграция с популярными LMS системами

### Долгосрочная перспектива (6-12 месяцев)
- Multi-tenancy для SaaS модели
- Горизонтальное масштабирование через Kubernetes
- ML pipeline для fine-tuning моделей на пользовательских данных
- Marketplace для обмена учебными материалами

## 7. Компромиссы и приоритизация

### Матрица приоритетов
1. **КРИТИЧЕСКИЕ**: Безопасность, Наблюдаемость, Качество контента
2. **ВЫСОКИЕ**: Отказоустойчивость, Удобство использования, Модульность
3. **СРЕДНИЕ**: Использование ресурсов, Совместимость, Тестируемость
4. **НИЗКИЕ**: Производительность, Масштабируемость (на текущем этапе)

### Архитектурные trade-offs
- **Качество vs Скорость**: Приоритет на качество генерации
- **Простота vs Функциональность**: Баланс для технических пользователей
- **Локальность vs Масштабируемость**: Фокус на локальное развертывание с заделом на будущее
- **Безопасность vs Удобство**: Безопасность без ущерба для UX

## 8. Соответствие стандартам

- **ISO/IEC 25010:2011** - Модель качества систем и программного обеспечения
- **OWASP** - Практики безопасной разработки
- **12-Factor App** - Принципы для SaaS приложений (частичное соответствие)
- **OpenTelemetry** - Стандарты наблюдаемости (в перспективе)

## Заключение

Данный документ определяет нефункциональные требования для LearnFlow AI с учетом текущего статуса проекта как локально развертываемого решения и перспектив развития в SaaS платформу. Приоритеты могут пересматриваться по мере эволюции проекта и получения обратной связи от сообщества.